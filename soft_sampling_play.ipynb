{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fec8c7c-5c5b-4582-be0d-2aac483d46f1",
   "metadata": {},
   "source": [
    "# Soft Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f7f433c-c671-4122-a066-765af2bb0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, tensor\n",
    "from jaxtyping import Float, Int\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False); # disable backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a212d0-a163-4b64-b42d-1a4698064fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "local_files_only = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# model_name = \"gpt2\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    local_files_only=local_files_only,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=local_files_only)\n",
    "tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "\n",
    "if model_name == \"gpt2\":\n",
    "    context_length = model.config.n_ctx\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "else: # llama\n",
    "    context_length = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa29f7a6-3f24-40be-9146-1baf44484c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:\n",
      "tensor([[128000,   1820,   8415,   7731,    304,    279,   9072],\n",
      "        [128000,  15339,  27394,    856,   2362,   4333, 128004],\n",
      "        [128000,   3594,  25936,   1554,   9451,  25936, 128004]],\n",
      "       device='mps:0')\n",
      "Decode:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the cat sat in the hat',\n",
       " 'hello darkness my old friend',\n",
       " 'soft sampling smosh sampling']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(inputs) -> Int[Tensor, \"bs seq\"]:\n",
    "    return tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=context_length\n",
    "    )[\"input_ids\"].to(device)\n",
    "    \n",
    "def decode(tokens) -> str:\n",
    "    return tokenizer.batch_decode(tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "prompts = [\"the cat sat in the hat\", \"hello darkness my old friend\", \"soft sampling smosh sampling\"]\n",
    "tokens = tokenize(prompts)\n",
    "print(\"Tokenized:\")\n",
    "print(tokens)\n",
    "print(\"Decode:\")\n",
    "decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8295968-8148-4f61-8812-a55cf4e416a0",
   "metadata": {},
   "source": [
    "## Mock Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9beff69-2650-43bc-94b8-45ec54d67b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 27.4418\n",
      "'' : '' / ''\n",
      "'' : 'the' / 'Question'\n",
      "'the' : ' cat' / ']:\\n\\n'\n",
      "'the cat' : ' sat' / 'Question'\n",
      "'the cat sat' : ' in' / '.mark'\n",
      "'the cat sat in' : ' the' / 'ing'\n",
      "'the cat sat in the' : ' hat' / 'ose'\n",
      "'the cat sat in the hat' : '' / 'Random'\n",
      "---\n",
      "'' : '' / ''\n",
      "'' : 'where' / 'When'\n",
      "'where' : ' did' / '\\n'\n",
      "'where did' : ' all' / 'Question'\n",
      "'where did all' : ' the' / '\\n\\n'\n",
      "'where did all the' : ' hats' / 'on'\n",
      "'where did all the hats' : ' go' / ' ='\n",
      "'where did all the hats go' : '?' / '\\n\\n'\n",
      "---\n",
      "'' : '' / ''\n",
      "'' : 'I' / 'Question'\n",
      "'I' : ' am' / '_'\n",
      "'I am' : ' the' / 'Question'\n",
      "'I am the' : ' r' / '_S'\n",
      "'I am the r' : 'izz' / ' C'\n",
      "'I am the rizz' : 'ler' / 'uder'\n",
      "'I am the rizzler' : '' / 'E'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "def to_cpu(t): return t.detach().cpu()\n",
    "\n",
    "def mk_proba_dist(\n",
    "    logits, # (batch_size, d_vocab)\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    min_p=None,\n",
    "):\n",
    "    \"Create probability distribution from logits\"\n",
    "    batch_size, d_vocab = logits.shape\n",
    "    device = logits.device\n",
    "    \n",
    "    if top_k is not None:\n",
    "        top_logits, top_idxs = logits.topk(top_k, dim=-1)\n",
    "        logits = (torch.full_like(logits, float('-inf'))\n",
    "                       .scatter_(1, top_idxs, top_logits))\n",
    "\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "    if min_p is not None:\n",
    "        max_p = probs.max(-1, keepdim=True).values\n",
    "        mask = probs >= (max_p * min_p)\n",
    "        probs = probs * mask\n",
    "        probs = probs / probs.sum(dim=-1, keepdim=True) # renormalize\n",
    "\n",
    "    assert logits.shape == probs.shape\n",
    "    return probs\n",
    "\n",
    "@delegates(mk_proba_dist)\n",
    "def soft_sampling_forward(\n",
    "    model,\n",
    "    input_ids, # tokens of shape (batch_size, seq_len)\n",
    "    guidance_alpha, # guidance weighting -- 1 equivalent to discrete sampling\n",
    "    **kwargs, # passed to mk_proba_dist\n",
    "):\n",
    "    \"Single train step using soft sampling\"\n",
    "    assert 0 <= guidance_alpha <= 1\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    device = input_ids.device\n",
    "\n",
    "    # cache\n",
    "    past_key_values = None\n",
    "    # position_ids = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
    "\n",
    "    W_E = model.get_input_embeddings().weight\n",
    "\n",
    "    loss = torch.tensor(0., device=device)\n",
    "    embeds = W_E[input_ids[:, :1]]  # BOS shape: (batch_size, 1, d_model)\n",
    "    tokens = [ to_cpu(input_ids[:, :1]) ]\n",
    "    for t in range(1, seq_len):\n",
    "        outputs = model(\n",
    "            inputs_embeds=embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            # position_ids=position_ids,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        logits_t = outputs.logits[:, -1]\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        p_t = mk_proba_dist(logits_t, **kwargs)\n",
    "\n",
    "        # loss\n",
    "        loss_t = F.cross_entropy(p_t, input_ids[:, t])\n",
    "        loss += loss_t\n",
    "\n",
    "        # discrete sample -- for logging\n",
    "        next_tokens = torch.multinomial(p_t, 1) # (batch_size, 1)\n",
    "        tokens.append(to_cpu(next_tokens))\n",
    "\n",
    "        # soft sample\n",
    "        next_emb_soft = p_t @ W_E      # soft sampling\n",
    "        next_emb_gt = W_E[input_ids[:, t]] # guidance sampling\n",
    "\n",
    "        next_embed = (\n",
    "            guidance_alpha * next_emb_gt +\n",
    "            (1 - guidance_alpha) * next_emb_soft\n",
    "        )\n",
    "        embeds = torch.cat([embeds, next_embed[:, None, :]], dim=1)\n",
    "        # position_ids += 1\n",
    "\n",
    "    tokens = torch.cat(tokens, dim=1)\n",
    "    # normalize: mean batch_size, sum seq_len\n",
    "    loss = loss / batch_size\n",
    "    return loss, tokens\n",
    "\n",
    "prompts = [\n",
    "    \"the cat sat in the hat\",\n",
    "    \"where did all the hats go?\",\n",
    "    \"I am the rizzler\",\n",
    "]\n",
    "\n",
    "batch = tokenize(prompts)\n",
    "loss, tokens = soft_sampling_forward(\n",
    "    model,\n",
    "    input_ids=batch,\n",
    "    guidance_alpha=0.,\n",
    "    temperature=1.,\n",
    "    min_p=None,\n",
    ")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "inp_tokens =  [decode(o) for o in batch]\n",
    "outp_tokens = [decode(o) for o in tokens]\n",
    "\n",
    "for inp, outp in zip(inp_tokens, outp_tokens):\n",
    "    prev_toks = \"\"\n",
    "    for i_t, o_t in zip(inp, outp):\n",
    "        print(f\"{repr(prev_toks)} : {repr(i_t)} / {repr(o_t)}\")\n",
    "        prev_toks += i_t\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9626df79-001a-457d-b3b8-c787e102ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from dataclasses import dataclass, asdict, field\n",
    "\n",
    "@dataclass\n",
    "class ProbabilityDistributionArguments:\n",
    "    temperature: float = 1.\n",
    "    top_k: int | None = None\n",
    "    min_p: float | None = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        assert self.temperature >= 0\n",
    "        assert self.top_k is None or self.min_p > 0\n",
    "        assert self.min_p is None or 0 <= self.min_p <= 1\n",
    "\n",
    "@dataclass\n",
    "class GuidanceAlphaScheduler:\n",
    "    guidance_alpha_warmup_delay_ratio: float = 0.1\n",
    "    guidance_alpha_warmup_ratio: float = 0.7\n",
    "    guidance_alpha_max: float = 0.7\n",
    "    \n",
    "    def get_scheduler(self, num_training_steps: int) -> Callable[[int], float]:\n",
    "        warmup_delay_steps = int(num_training_steps * self.guidance_alpha_warmup_delay_ratio)\n",
    "        warmup_end_steps = int(num_training_steps * (self.guidance_alpha_warmup_delay_ratio + self.guidance_alpha_warmup_ratio))\n",
    "        \n",
    "        def scheduler(step: int) -> float:\n",
    "            if step < warmup_delay_steps:\n",
    "                return 0.0\n",
    "            elif step < warmup_end_steps:\n",
    "                # Linear warmup from 0 to guidance_alpha_max\n",
    "                warmup_progress = (step - warmup_delay_steps) / (warmup_end_steps - warmup_delay_steps)\n",
    "                return self.guidance_alpha_max * warmup_progress\n",
    "            else:\n",
    "                return self.guidance_alpha_max\n",
    "            \n",
    "        return scheduler\n",
    "\n",
    "@dataclass\n",
    "class SoftDecodingTrainingArguments(TrainingArguments):\n",
    "    guidance_alpha_scheduler: GuidanceAlphaScheduler = field(default_factory=lambda: GuidanceAlphaScheduler())\n",
    "    probability_distribution_argments: ProbabilityDistributionArguments = field(default_factory=lambda: ProbabilityDistributionArguments())\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        if isinstance(self.probability_distribution_argments, dict):\n",
    "            self.probability_distribution_argments = ProbabilityDistributionArguments(**self.probability_distribution_argments)\n",
    "        if isinstance(self.guidance_alpha_scheduler, dict):\n",
    "            self.guidance_alpha_scheduler = GuidanceAlphaScheduler(**self.guidance_alpha_scheduler)\n",
    "    \n",
    "\n",
    "class SoftDecodingTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        args: SoftDecodingTrainingArguments,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(model, args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        loss, predicted_tokens = soft_sampling_forward(\n",
    "            model=model,\n",
    "            input_ids=input_ids,\n",
    "            guidance_alpha=self.guidance_alpha,\n",
    "            **asdict(self.args.probability_distribution_argments),\n",
    "        )\n",
    "        return (\n",
    "            (loss, {\"predicted_tokens\": predicted_tokens})\n",
    "            if return_outputs\n",
    "            else loss\n",
    "        )\n",
    "            \n",
    "    def create_scheduler(self, num_training_steps: int, optimizer=None):\n",
    "        self.guidance_alpha_scheduler = self.args.guidance_alpha_scheduler.get_scheduler(num_training_steps)\n",
    "        return super().create_scheduler(num_training_steps, optimizer)\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        self.guidance_alpha = self.guidance_alpha_scheduler(self.state.global_step)\n",
    "        if self.control.should_log: self.log(\"guidance_alpha\", self.guidance_alpha, prog_bar=True)\n",
    "        return super().training_step(model, inputs)\n",
    "\n",
    "\n",
    "training_args = SoftDecodingTrainingArguments(\n",
    "    output_dir=\"/tmp/soft-decoding-output\",\n",
    "    guidance_alpha_scheduler=dict(\n",
    "        guidance_alpha_warmup_delay_ratio=0.1,\n",
    "        guidance_alpha_warmup_ratio=0.7,\n",
    "        guidance_alpha_max=0.7\n",
    "    ),\n",
    "    probability_distribution_argments=dict(\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "        min_p=0.2\n",
    "    )\n",
    ")\n",
    "# trainer = SoftDecodingTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "# )\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
