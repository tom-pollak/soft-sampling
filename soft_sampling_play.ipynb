{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fec8c7c-5c5b-4582-be0d-2aac483d46f1",
   "metadata": {},
   "source": [
    "# Soft Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f7f433c-c671-4122-a066-765af2bb0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, tensor\n",
    "from jaxtyping import Float, Int\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# torch.set_grad_enabled(False); # disable backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a212d0-a163-4b64-b42d-1a4698064fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "{'input_ids': [128000, 128004], 'attention_mask': [1, 1]}\n",
      "['<|begin_of_text|>', '<|finetune_right_pad_id|>']\n"
     ]
    }
   ],
   "source": [
    "local_files_only = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# model_name = \"gpt2\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    local_files_only=local_files_only,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=local_files_only)\n",
    "\n",
    "print(tokenizer(\"<|finetune_right_pad_id|>\"))\n",
    "print(tokenizer.convert_ids_to_tokens([128000, 128004]))\n",
    "\n",
    "tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "\n",
    "if model_name == \"gpt2\":\n",
    "    context_length = model.config.n_ctx\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "else: # llama\n",
    "    context_length = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fedfd5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128256, 128257]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa29f7a6-3f24-40be-9146-1baf44484c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:\n",
      "tensor([[128000,   1820,   8415,   7731,    304,    279,   9072],\n",
      "        [128000,  15339,  27394,    856,   2362,   4333, 128004],\n",
      "        [128000,   3594,  25936,   1554,   9451,  25936, 128004]],\n",
      "       device='mps:0')\n",
      "Decode:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the cat sat in the hat',\n",
       " 'hello darkness my old friend',\n",
       " 'soft sampling smosh sampling']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(inputs) -> Int[Tensor, \"bs seq\"]:\n",
    "    return tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=context_length\n",
    "    )[\"input_ids\"].to(device)\n",
    "\n",
    "def decode(tokens) -> str:\n",
    "    return tokenizer.batch_decode(tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "prompts = [\"the cat sat in the hat\", \"hello darkness my old friend\", \"soft sampling smosh sampling\"]\n",
    "tokens = tokenize(prompts)\n",
    "print(\"Tokenized:\")\n",
    "print(tokens)\n",
    "print(\"Decode:\")\n",
    "decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8295968-8148-4f61-8812-a55cf4e416a0",
   "metadata": {},
   "source": [
    "## Mock Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc603430-efe3-465b-a724-ebe8ba045e85",
   "metadata": {},
   "source": [
    "Freeze everything but the head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dced64b7-af16-4b99-88ff-38f6edf740f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.lm_head.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9beff69-2650-43bc-94b8-45ec54d67b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 27.4427\n",
      "'' : '' / ''\n",
      "'' : 'the' / 'Question'\n",
      "'the' : ' cat' / 'Site'\n",
      "'the cat' : ' sat' / 'British'\n",
      "'the cat sat' : ' in' / 'Exception'\n",
      "'the cat sat in' : ' the' / 'lene'\n",
      "'the cat sat in the' : ' hat' / '\">'\n",
      "'the cat sat in the hat' : '' / '\\n'\n",
      "---\n",
      "'' : '' / ''\n",
      "'' : 'where' / 'Question'\n",
      "'where' : ' did' / ':\\n'\n",
      "'where did' : ' all' / '男'\n",
      "'where did all' : ' the' / 'Comment'\n",
      "'where did all the' : ' hats' / 'Request'\n",
      "'where did all the hats' : ' go' / 'De'\n",
      "'where did all the hats go' : '?' / '\",\\n'\n",
      "---\n",
      "'' : '' / ''\n",
      "'' : 'I' / '['\n",
      "'I' : ' am' / 'Nr'\n",
      "'I am' : ' the' / ' Tag'\n",
      "'I am the' : ' r' / '_question'\n",
      "'I am the r' : 'izz' / 'ster'\n",
      "'I am the rizz' : 'ler' / 'isms'\n",
      "'I am the rizzler' : '' / ' \"/'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "def to_cpu(t): return t.detach().cpu()\n",
    "\n",
    "def mk_proba_dist(\n",
    "    logits, # (batch_size, d_vocab)\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    min_p=None,\n",
    "):\n",
    "    \"Create probability distribution from logits\"\n",
    "    batch_size, d_vocab = logits.shape\n",
    "    device = logits.device\n",
    "\n",
    "    if top_k is not None:\n",
    "        top_logits, top_idxs = logits.topk(top_k, dim=-1)\n",
    "        logits = (torch.full_like(logits, float('-inf'))\n",
    "                       .scatter_(1, top_idxs, top_logits))\n",
    "\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "    if min_p is not None:\n",
    "        max_p = probs.max(-1, keepdim=True).values\n",
    "        mask = probs >= (max_p * min_p)\n",
    "        probs = probs * mask\n",
    "        probs = probs / probs.sum(dim=-1, keepdim=True) # renormalize\n",
    "\n",
    "    assert logits.shape == probs.shape\n",
    "    return probs\n",
    "\n",
    "@delegates(mk_proba_dist)\n",
    "def soft_sampling_forward(\n",
    "    model,\n",
    "    input_ids, # tokens of shape (batch_size, seq_len)\n",
    "    guidance_alpha, # guidance weighting -- 1 equivalent to discrete sampling\n",
    "    **kwargs, # passed to mk_proba_dist\n",
    "):\n",
    "    \"Single train step using soft sampling\"\n",
    "    assert 0 <= guidance_alpha <= 1\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    device = input_ids.device\n",
    "\n",
    "    # cache\n",
    "    past_key_values = None\n",
    "    # position_ids = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
    "\n",
    "    W_E = model.get_input_embeddings().weight\n",
    "\n",
    "    loss = torch.tensor(0., device=device)\n",
    "    embeds = W_E[input_ids[:, :1]]  # BOS shape: (batch_size, 1, d_model)\n",
    "    tokens = [ to_cpu(input_ids[:, :1]) ]\n",
    "    for t in range(1, seq_len):\n",
    "        outputs = model(\n",
    "            inputs_embeds=embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            # position_ids=position_ids,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        logits_t = outputs.logits[:, -1]\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # loss\n",
    "        loss_t = F.cross_entropy(logits_t, input_ids[:, t])\n",
    "        loss += loss_t\n",
    "\n",
    "        p_t = mk_proba_dist(logits_t, **kwargs)\n",
    "\n",
    "        # discrete sample -- for logging\n",
    "        next_tokens = torch.multinomial(p_t, 1) # (batch_size, 1)\n",
    "        tokens.append(to_cpu(next_tokens))\n",
    "\n",
    "        # soft sample\n",
    "        next_emb_soft = p_t @ W_E      # soft sampling\n",
    "        next_emb_gt = W_E[input_ids[:, t]] # guidance sampling\n",
    "\n",
    "        next_embed = (\n",
    "            guidance_alpha * next_emb_gt +\n",
    "            (1 - guidance_alpha) * next_emb_soft\n",
    "        )\n",
    "        embeds = torch.cat([embeds, next_embed[:, None, :]], dim=1)\n",
    "        # position_ids += 1\n",
    "\n",
    "    tokens = torch.cat(tokens, dim=1)\n",
    "    # normalize loss\n",
    "    loss = loss / (batch_size * (seq_len - 1))\n",
    "    return loss, tokens\n",
    "\n",
    "prompts = [\n",
    "    \"the cat sat in the hat\",\n",
    "    \"where did all the hats go?\",\n",
    "    \"I am the rizzler\",\n",
    "]\n",
    "\n",
    "batch = tokenize(prompts)\n",
    "loss, tokens = soft_sampling_forward(\n",
    "    model,\n",
    "    input_ids=batch,\n",
    "    guidance_alpha=0.,\n",
    "    temperature=1.,\n",
    "    min_p=None,\n",
    ")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "inp_tokens =  [decode(o) for o in batch]\n",
    "outp_tokens = [decode(o) for o in tokens]\n",
    "\n",
    "for inp, outp in zip(inp_tokens, outp_tokens):\n",
    "    prev_toks = \"\"\n",
    "    for i_t, o_t in zip(inp, outp):\n",
    "        print(f\"{repr(prev_toks)} : {repr(i_t)} / {repr(o_t)}\")\n",
    "        prev_toks += i_t\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9626df79-001a-457d-b3b8-c787e102ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from dataclasses import dataclass, asdict, field\n",
    "\n",
    "@dataclass\n",
    "class ProbabilityDistributionArguments:\n",
    "    temperature: float = 1.\n",
    "    top_k: int | None = None\n",
    "    min_p: float | None = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.temperature >= 0\n",
    "        assert self.top_k is None or self.min_p > 0\n",
    "        assert self.min_p is None or 0 <= self.min_p <= 1\n",
    "\n",
    "@dataclass\n",
    "class GuidanceAlphaScheduler:\n",
    "   warmup_delay_ratio: float = 0.1  # Start warmup after this % of steps\n",
    "   warmup_ratio: float = 0.7        # Warmup duration as % of steps\n",
    "   min_alpha: float = 0.25          # Minimum guidance alpha value\n",
    "\n",
    "   def get_scheduler(self, num_steps: int) -> Callable[[int], float]:\n",
    "       \"Scheduler decays from 1.0 to min_alpha\"\n",
    "       delay_end = int(num_steps * self.warmup_delay_ratio)\n",
    "       warmup_end = int(num_steps * (self.warmup_delay_ratio + self.warmup_ratio))\n",
    "\n",
    "       def scheduler(step: int) -> float:\n",
    "           if step < delay_end:\n",
    "               return 1.0\n",
    "           if step < warmup_end:\n",
    "               progress = (step - delay_end) / (warmup_end - delay_end)\n",
    "               return 1.0 - ((1.0 - self.min_alpha) * progress)\n",
    "           return self.min_alpha\n",
    "\n",
    "       return scheduler\n",
    "\n",
    "@dataclass\n",
    "class SoftDecodingTrainingArguments(TrainingArguments):\n",
    "    guidance_alpha_scheduler: GuidanceAlphaScheduler = field(default_factory=lambda: GuidanceAlphaScheduler())\n",
    "    probability_distribution_argments: ProbabilityDistributionArguments = field(default_factory=lambda: ProbabilityDistributionArguments())\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        if isinstance(self.probability_distribution_argments, dict):\n",
    "            self.probability_distribution_argments = ProbabilityDistributionArguments(**self.probability_distribution_argments)\n",
    "        if isinstance(self.guidance_alpha_scheduler, dict):\n",
    "            self.guidance_alpha_scheduler = GuidanceAlphaScheduler(**self.guidance_alpha_scheduler)\n",
    "\n",
    "\n",
    "class SoftDecodingTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        args: SoftDecodingTrainingArguments,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(model, args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        loss, predicted_tokens = soft_sampling_forward(\n",
    "            model=model,\n",
    "            input_ids=input_ids,\n",
    "            guidance_alpha=self.guidance_alpha,\n",
    "            **asdict(self.args.probability_distribution_argments),\n",
    "        )\n",
    "        return (\n",
    "            (loss, {\"predicted_tokens\": predicted_tokens})\n",
    "            if return_outputs\n",
    "            else loss\n",
    "        )\n",
    "\n",
    "    def create_scheduler(self, num_training_steps: int, optimizer=None):\n",
    "        self.guidance_alpha_scheduler = self.args.guidance_alpha_scheduler.get_scheduler(num_training_steps)\n",
    "        return super().create_scheduler(num_training_steps, optimizer)\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        self.guidance_alpha = self.guidance_alpha_scheduler(self.state.global_step)\n",
    "        if self.control.should_log: self.log(\"guidance_alpha\", self.guidance_alpha, prog_bar=True)\n",
    "        return super().training_step(model, inputs)\n",
    "\n",
    "\n",
    "training_args = SoftDecodingTrainingArguments(\n",
    "    output_dir=\"/tmp/soft-decoding-output\",\n",
    "    guidance_alpha_scheduler=dict(\n",
    "        warmup_delay_ratio=0.1,\n",
    "        warmup_ratio=0.7,\n",
    "        min_alpha=0.3\n",
    "    ),\n",
    "    probability_distribution_argments=dict(\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "        min_p=0.2\n",
    "    )\n",
    ")\n",
    "# trainer = SoftDecodingTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d097b7d-0d4e-4e33-b95f-865a17338eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "class SoftGRPOTrainer(GRPOTrainer):\n",
    "    \"\"\"\n",
    "    Subclass the GRPOTrainer to override the generation step with a custom\n",
    "    'soft sampling' method.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_completions(self, prompts, num_completions=1, **kwargs):\n",
    "        \"\"\"\n",
    "        Called by GRPOTrainer internally to produce completions for each prompt.\n",
    "        Here, we override to do 'soft decoding' with manual forward passes.\n",
    "\n",
    "        `prompts`: list of prompt strings or list of message dicts (depending on your format).\n",
    "        `num_completions`: how many completions to produce per prompt (the 'G' in GRPO).\n",
    "\n",
    "        Return format: A list of length (batch_size * num_completions), where each item\n",
    "                       is either a string or a chat-style list of messages.\n",
    "        \"\"\"\n",
    "        # Basic code: we’ll handle each prompt individually, produce num_completions for each.\n",
    "        # This is naive. You might want to do batched decoding. For clarity, we do it prompt-by-prompt.\n",
    "        completions = []\n",
    "        for prompt in prompts:\n",
    "            for _ in range(num_completions):\n",
    "                completion_text = self._soft_decode_one(prompt, **kwargs)\n",
    "                completions.append(completion_text)\n",
    "\n",
    "        return completions\n",
    "\n",
    "    def _soft_decode_one(self, prompt, max_length=128, temperature=1.0, guidance_alpha=0.0):\n",
    "        \"\"\"\n",
    "        Single forward pass that does 'soft sampling' for one prompt.\n",
    "        Here, guidance_alpha=0 => purely soft. guidance_alpha=1 => purely discrete teacher-forcing style.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Tokenize prompt\n",
    "        if isinstance(prompt, list):\n",
    "            # If using Chat format with messages, flatten them. Or handle them however you want.\n",
    "            # For simplicity, assume prompt[-1][\"content\"] is your user content:\n",
    "            prompt_str = prompt[-1][\"content\"]\n",
    "        else:\n",
    "            prompt_str = prompt\n",
    "\n",
    "        # Get tokenizer from the trainer's model\n",
    "        tokenizer = self.tokenizer\n",
    "        model = self.model  # the policy model we are training (should be half or fp16)\n",
    "        device = model.device\n",
    "\n",
    "        input_ids = tokenizer.encode(prompt_str, return_tensors=\"pt\").to(device)\n",
    "        # We'll keep track of discrete tokens for logging/decoding:\n",
    "        all_tokens = input_ids.clone()\n",
    "\n",
    "        # 2. We'll gather the last hidden state or next-token logits in a loop:\n",
    "        past_key_values = None\n",
    "        # Convert the initial prompt tokens into embeddings:\n",
    "        W_E = model.get_input_embeddings().weight  # shape [vocab_size, hidden_dim]\n",
    "        embeds = W_E[input_ids[:, -1]]  # last token embedding. shape: [1, hidden_dim]\n",
    "\n",
    "        # We have to manually feed the entire prefix or do some caching trick.\n",
    "        # For simplicity, re-run from scratch each step. For efficiency, you'd do caching more carefully.\n",
    "\n",
    "        # But let's do a simple approach: run the entire input_ids once for the prompt, then keep a loop.\n",
    "        # We can do that by calling `model(..., use_cache=True)`.\n",
    "        # Then on subsequent tokens, we only feed the newly appended embedding.\n",
    "\n",
    "        # However, let's illustrate a simple step-by-step (inefficient) approach for clarity.\n",
    "\n",
    "        # Step 1: encode the entire prompt in a normal forward pass:\n",
    "        out = model(input_ids=input_ids, use_cache=True)\n",
    "        past_key_values = out.past_key_values\n",
    "\n",
    "        # Now we loop for next tokens\n",
    "        for step in range(max_length):\n",
    "            # Obtain next-token logits from the last forward pass\n",
    "            # Usually, out.logits is shape [batch_size, seq_len, vocab_size].\n",
    "            # We want the final step's logits.\n",
    "            logits = out.logits[:, -1, :]  # shape [1, vocab_size]\n",
    "\n",
    "            # 1) Build a distribution over the next token\n",
    "            #    shape [1, vocab_size]\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            # 2) Optionally sample a discrete token for logging\n",
    "            #    shape [1, 1]\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # 3) \"Soft\" next embedding\n",
    "            # shape [1, hidden_dim]\n",
    "            emb_soft = probs @ W_E  # [1, vocab_size] * [vocab_size, hidden_dim]\n",
    "\n",
    "            # 4) Possibly do a “guidance mix” with the ground-truth token (if training with supervised data),\n",
    "            #    but in typical RL setting we don’t have that. So just use `emb_soft`.\n",
    "            # next_emb = guidance_alpha * W_E[true_token_id] + (1 - guidance_alpha) * emb_soft\n",
    "            next_emb = emb_soft\n",
    "\n",
    "            # 5) If next_token_id is EOS, break\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                # append for final decode\n",
    "                all_tokens = torch.cat([all_tokens, next_token_id], dim=-1)\n",
    "                break\n",
    "\n",
    "            # Otherwise, append the discrete token to all_tokens for logging/decoding\n",
    "            all_tokens = torch.cat([all_tokens, next_token_id], dim=-1)\n",
    "\n",
    "            # 6) Feed the new embedding into the model:\n",
    "            out = model(inputs_embeds=next_emb.unsqueeze(1),  # shape [1, 1, hidden_dim]\n",
    "                        past_key_values=past_key_values,\n",
    "                        use_cache=True)\n",
    "            past_key_values = out.past_key_values\n",
    "\n",
    "        # 7) Convert final tokens to string\n",
    "        completion_str = tokenizer.decode(all_tokens[0, :], skip_special_tokens=True)\n",
    "        return completion_str\n",
    "\n",
    "\n",
    "#\n",
    "# Then you can use this `SoftGRPOTrainer` similarly to how you would use GRPOTrainer:\n",
    "#\n",
    "\n",
    "def train_soft_grpo():\n",
    "    # Suppose you have your dataset & reward functions\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"trl-lib/tldr\", split=\"train\")  # trivial example\n",
    "\n",
    "    def reward_len(completions, **kwargs):\n",
    "        return [-abs(20 - len(c)) for c in completions]\n",
    "\n",
    "    # Basic config\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=\"my-soft-grpo-checkpoints\",\n",
    "        logging_steps=10,\n",
    "        use_vllm=False,  # must disable vLLM for custom \"soft\" generation\n",
    "    )\n",
    "\n",
    "    trainer = SoftGRPOTrainer(\n",
    "        model=\"Qwen/Qwen2-0.5B-Instruct\",\n",
    "        reward_funcs=reward_len,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "#\n",
    "# That’s it: now your completions come from `_soft_decode_one()` instead of the standard discrete generate().\n",
    "#\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
